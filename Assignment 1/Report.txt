Assignment 1 Report

Name: Anish Amul Vaidya

1. Performance on the development data with 100% of the training data
1a. spam precision: 0.9912925170068028
1b. spam recall: 0.9748461332619749 
1c. spam F1 score: 0.9830005396654075
1d. ham precision: 0.9373333333333334 
1e. ham recall: 0.9777468706536857 
1f. ham F1 score: 0.9571136827773997

2. Performance on the development data with 10% of the training data
2a. spam precision: 0.9903314917127072 
2b. spam recall: 0.9755102040816327 
2c. spam F1 score: 0.9828649760109665
2d. ham precision: 0.9421221864951769
2e. ham recall: 0.9766666666666667
2f. ham F1 score: 0.9590834697217676

3. Description of enhancement(s) you tried (e.g., different approach(es) to smoothing, treating common words differently, dealing with unknown words differently):

Modifications done:
    a. Include stopwords and ignore the words from the dataset if they are in stopwords list. Stopwords list was acquired from NLTK English
    stopwords list.
    b. Use Add-K smoothing instead of Add-1 smoothing. The value of K used was K = 0.75
    c. Remove the words from vocabulary if they occur in less than 5 documents (ie. text files), ie. remove word from vocabulary if
    count(word_in_number_of_documents) < 5.
    d. Club all the numeric values together into a single tag "NUM".
    Below are the best performance results I got after enhancements.

4. Best performance results based on enhancements. Note that these could be the same or worse than the standard implementation.
4a. spam precision: 0.9610321221695629
4b. spam recall: 0.9931972789115646 
4c. spam F1 score: 0.9768499933092465
4d. ham precision: 0.9818445896877269
4e. ham recall: 0.9013333333333333
4f. ham F1 score: 0.9398679179701077
